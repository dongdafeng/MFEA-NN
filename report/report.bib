@article{ednn,
  author    = {Risto Miikkulainen and
               Jason Zhi Liang and
               Elliot Meyerson and
               Aditya Rawal and
               Dan Fink and
               Olivier Francon and
               Bala Raju and
               Hormoz Shahrzad and
               Arshak Navruzyan and
               Nigel Duffy and
               Babak Hodjat},
  title     = {Evolving Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1703.00548},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.00548},
  archivePrefix = {arXiv},
  eprint    = {1703.00548},
  timestamp = {Wed, 07 Jun 2017 14:42:44 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/MiikkulainenLMR17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@InProceedings{mfea-ffnn,
author="Chandra, Rohitash
and Gupta, Abhishek
and Ong, Yew-Soon
and Goh, Chi-Keong",
editor="Hirose, Akira
and Ozawa, Seiichi
and Doya, Kenji
and Ikeda, Kazushi
and Lee, Minho
and Liu, Derong",
title="Evolutionary Multi-task Learning for Modular Training of Feedforward Neural Networks",
booktitle="Neural Information Processing",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="37--46",
abstract="Multi-task learning enables learning algorithms to harness shared knowledge from several tasks in order to provide better performance. In the past, neuro-evolution has shownpromising performance for a number of real-world applications. Recently, evolutionary multi-tasking has been proposed for optimisation problems. In this paper, we present a multi-task learning for neural networks that evolves modular network topologies. In the proposed method, each task is defined by a specific network topology defined with a different number of hidden neurons. The method produces a modular network that could be effective even if some of the neurons and connections are removed from selected trained modules in the network. We demonstrate the effectiveness of the method using feedforward networks to learn selected n-bit parity problems of varying levels of difficulty. The results show better training and generalisation performance when the modules for representing additional knowledge are added by increasing hidden neurons during training.",
isbn="978-3-319-46672-9"
}
@article{mfea,
  title={Multifactorial Evolution: Toward Evolutionary Multitasking},
  author={Abhishek Gupta and Yew-Soon Ong and Liang Feng},
  journal={IEEE Transactions on Evolutionary Computation},
  year={2016},
  volume={20},
  pages={343-357}
}
@article{adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  journal   = {CoRR},
  volume    = {abs/1412.6980},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.6980},
  archivePrefix = {arXiv},
  eprint    = {1412.6980},
  timestamp = {Wed, 07 Jun 2017 14:40:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{sbx,
author = {Bhusan Agrawal, Ram and Deb, Kalyanmoy and Bhushan Agrawal, Ram},
year = {2000},
month = {06},
pages = {},
title = {Simulated Binary Crossover for Continuous Search Space},
volume = {9},
booktitle = {Complex Systems}
}
@article{polinomial-mutate,
 author = {Deb, Kalyanmoy and Deb, Debayan},
 title = {Analysing Mutation Schemes for Real-parameter Genetic Algorithms},
 journal = {Int. J. Artif. Intell. Soft Comput.},
 issue_date = {February 2014},
 volume = {4},
 number = {1},
 month = feb,
 year = {2014},
 issn = {1755-4950},
 pages = {1--28},
 numpages = {28},
 url = {http://dx.doi.org/10.1504/IJAISC.2014.059280},
 doi = {10.1504/IJAISC.2014.059280},
 acmid = {2582524},
 publisher = {Inderscience Publishers},
 address = {Inderscience Publishers, Geneva, SWITZERLAND},
} 

